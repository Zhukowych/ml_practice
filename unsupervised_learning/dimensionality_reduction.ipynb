{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Projection** - we can use projection when all data points are close to some lower-dimensional hyper plane, then we can project data points to that dimension\n",
    "\n",
    "Sometimes we musn't have all degrees of freedom that our data has, because for example when we write numbers it assumes that we don't generate random data - thus \n",
    "given data can be projected to a **lower dimensional manifold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(10, 2)\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt[0]\n",
    "c2 = Vt[1]\n",
    "\n",
    "\n",
    "W2 = Vt[:2].T\n",
    "X2D = X_centered @ W2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosing the right number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", as_frame=False)\n",
    "X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
    "X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or it can be done as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(154)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_automatical = PCA(n_components=0.95)\n",
    "X_reduced = pca_automatical.fit_transform(X_train)\n",
    "pca_automatical.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incremental PCA** - split data into smaller batches and fit it into IPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "incremental_pca = IncrementalPCA(n_components=154)\n",
    "for batch in np.array_split(X_train, n_batches):\n",
    "    incremental_pca.partial_fit(batch)\n",
    "\n",
    "X_reduced = incremental_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomized projection**\n",
    "\n",
    "There is a theoretical results that suggests a minimal number of dimensions to be sure that a random projection to that dimensionality won't get rid of\n",
    "more that required number of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "m, e = 5_000, 0.1\n",
    "d = johnson_lindenstrauss_min_dim(m, eps=e)\n",
    "np.random.seed(42)\n",
    "n = 20_000\n",
    "P = np.random.randn(d, n) / np.sqrt(d)\n",
    "X = np.random.randn(m, n)\n",
    "X_reduced = X @ P.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally lunear embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_unrolled = lle.fit_transform(X_swiss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
